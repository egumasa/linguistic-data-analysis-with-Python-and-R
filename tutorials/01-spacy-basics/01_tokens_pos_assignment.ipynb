{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 1 Assignment: spaCy Basics\n",
    "## Exploring spaCy's Core Objects\n",
    "\n",
    "**Learning Goals:**\n",
    "- Load spaCy models and process text\n",
    "- Understand the relationship between `nlp`, `Doc`, and `Token` objects\n",
    "- Explore basic token attributes\n",
    "- Practice iterating through documents and tokens\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install and import spaCy. If you're running this in Colab, uncomment the first two lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these lines if running in Google Colab:\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(f\"spaCy version: {spacy.__version__}\")\n",
    "print(f\"Model loaded: {nlp.meta['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Your First spaCy Document\n",
    "\n",
    "Let's start by processing a simple sentence and exploring the resulting `Doc` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a sample text\n",
    "text = \"Natural language processing with spaCy is powerful and efficient.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Doc object: {doc}\")\n",
    "print(f\"Type of doc: {type(doc)}\")\n",
    "print(f\"Number of tokens: {len(doc)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: Process Your Own Text\n",
    "\n",
    "Create a variable called `my_text` with a sentence of your choice, process it with spaCy, and print basic information about the resulting document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your own text and process it\n",
    "my_text = \"\"  # Replace with your sentence\n",
    "my_doc = None  # Process my_text with nlp()\n",
    "\n",
    "# TODO: Print the text, document, and token count\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your solution\n",
    "assert len(my_text) > 0, \"Please provide some text\"\n",
    "assert my_doc is not None, \"Please process the text with nlp()\"\n",
    "assert len(my_doc) > 0, \"The document should contain tokens\"\n",
    "print(\"✅ Task 1.1 completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Exploring Tokens\n",
    "\n",
    "Now let's look at individual tokens and their attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine each token in our document\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"Token Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "for token in doc:\n",
    "    print(f\"Text: '{token.text}' | Lemma: '{token.lemma_}' | POS: '{token.pos_}' | Is Stop: {token.is_stop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Token Attribute Explorer\n",
    "\n",
    "Complete the function below to extract specific token attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tokens(text):\n",
    "    \"\"\"\n",
    "    Analyze tokens in a text and return lists of different attributes.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to analyze\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with lists of token attributes\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    result = {\n",
    "        'tokens': [],\n",
    "        'lemmas': [],\n",
    "        'pos_tags': [],\n",
    "        'is_alpha': [],\n",
    "        'is_stop': []\n",
    "    }\n",
    "    \n",
    "    # TODO: Fill in the lists by iterating through tokens\n",
    "    for token in doc:\n",
    "        result['tokens'].append(token.text)  # Example - complete the rest\n",
    "        # TODO: Add token.lemma_ to lemmas list\n",
    "        # TODO: Add token.pos_ to pos_tags list  \n",
    "        # TODO: Add token.is_alpha to is_alpha list\n",
    "        # TODO: Add token.is_stop to is_stop list\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test your function\n",
    "sample_text = \"I'm learning spaCy for NLP analysis!\"\n",
    "analysis = analyze_tokens(sample_text)\n",
    "\n",
    "# Display results as a DataFrame for easy viewing\n",
    "df = pd.DataFrame(analysis)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your solution\n",
    "test_analysis = analyze_tokens(\"Hello world!\")\n",
    "assert len(test_analysis['lemmas']) > 0, \"Please fill in the lemmas list\"\n",
    "assert len(test_analysis['pos_tags']) > 0, \"Please fill in the pos_tags list\"\n",
    "assert len(test_analysis['is_alpha']) > 0, \"Please fill in the is_alpha list\"\n",
    "assert len(test_analysis['is_stop']) > 0, \"Please fill in the is_stop list\"\n",
    "print(\"✅ Task 2.1 completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Filtering Tokens\n",
    "\n",
    "Often we want to filter tokens based on certain criteria. Let's practice this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1: Content Words Only\n",
    "\n",
    "Create a function that extracts only \"content words\" (words that are alphabetic and not stop words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_words(text):\n",
    "    \"\"\"\n",
    "    Extract content words (alphabetic, non-stop words) from text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        \n",
    "    Returns:\n",
    "        list: List of content words (lowercased)\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    content_words = []\n",
    "    \n",
    "    # TODO: Iterate through tokens and add content words\n",
    "    for token in doc:\n",
    "        # TODO: Check if token is alphabetic AND not a stop word\n",
    "        if ...  # Complete this condition\n",
    "            content_words.append(token.text.lower())\n",
    "    \n",
    "    return content_words\n",
    "\n",
    "# Test your function\n",
    "test_text = \"The quick brown fox jumps over the lazy dog in the park.\"\n",
    "content = extract_content_words(test_text)\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Content words: {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your solution\n",
    "test_content = extract_content_words(\"The cat sat on the mat.\")\n",
    "assert 'cat' in test_content, \"Should include content words like 'cat'\"\n",
    "assert 'sat' in test_content, \"Should include content words like 'sat'\"\n",
    "assert 'the' not in test_content, \"Should not include stop words like 'the'\"\n",
    "assert 'on' not in test_content, \"Should not include stop words like 'on'\"\n",
    "print(\"✅ Task 3.1 completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Document Statistics\n",
    "\n",
    "Let's create a comprehensive analysis function that provides various statistics about a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1: Document Analyzer\n",
    "\n",
    "Complete the function below to calculate various document statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_document(text):\n",
    "    \"\"\"\n",
    "    Provide comprehensive statistics about a text document.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with various statistics\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # TODO: Calculate these statistics\n",
    "    stats = {\n",
    "        'total_tokens': 0,           # Total number of tokens\n",
    "        'alphabetic_tokens': 0,      # Number of alphabetic tokens\n",
    "        'punctuation_tokens': 0,     # Number of punctuation tokens\n",
    "        'stop_words': 0,             # Number of stop words\n",
    "        'content_words': 0,          # Number of content words (alphabetic + non-stop)\n",
    "        'unique_lemmas': 0,          # Number of unique lemmas\n",
    "        'avg_token_length': 0.0      # Average token length\n",
    "    }\n",
    "    \n",
    "    # TODO: Implement the calculations\n",
    "    lemmas_set = set()  # To track unique lemmas\n",
    "    total_length = 0    # To calculate average length\n",
    "    \n",
    "    for token in doc:\n",
    "        stats['total_tokens'] += 1\n",
    "        \n",
    "        # TODO: Update other statistics based on token properties\n",
    "        # Hint: Use token.is_alpha, token.is_punct, token.is_stop\n",
    "        # Remember to add to lemmas_set and total_length\n",
    "    \n",
    "    # TODO: Calculate unique lemmas and average length\n",
    "    stats['unique_lemmas'] = len(lemmas_set)\n",
    "    if stats['total_tokens'] > 0:\n",
    "        stats['avg_token_length'] = total_length / stats['total_tokens']\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Test with sample text\n",
    "sample = \"Natural language processing is fascinating! It involves computational linguistics, machine learning, and artificial intelligence.\"\n",
    "stats = analyze_document(sample)\n",
    "\n",
    "print(\"Document Statistics:\")\n",
    "print(\"-\" * 30)\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your solution\n",
    "test_stats = analyze_document(\"Hello, world! This is a test.\")\n",
    "assert test_stats['total_tokens'] > 0, \"Should count total tokens\"\n",
    "assert test_stats['punctuation_tokens'] > 0, \"Should count punctuation\"\n",
    "assert test_stats['alphabetic_tokens'] > 0, \"Should count alphabetic tokens\"\n",
    "assert test_stats['avg_token_length'] > 0, \"Should calculate average length\"\n",
    "print(\"✅ Task 4.1 completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Real-world Application\n",
    "\n",
    "Let's apply what we've learned to analyze a longer piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample article text\n",
    "article = \"\"\"\n",
    "Artificial intelligence has revolutionized many industries in recent years. \n",
    "From healthcare to finance, AI systems are transforming how we work and live. \n",
    "Natural language processing, a subset of AI, enables computers to understand \n",
    "and generate human language. This technology powers chatbots, translation \n",
    "services, and text analysis tools. Machine learning algorithms continue to \n",
    "improve, making AI more accurate and efficient than ever before.\n",
    "\"\"\".strip()\n",
    "\n",
    "print(\"Article Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Text: {article[:100]}...\")\n",
    "print()\n",
    "\n",
    "# Analyze the article\n",
    "article_stats = analyze_document(article)\n",
    "content_words = extract_content_words(article)\n",
    "\n",
    "print(\"Statistics:\")\n",
    "for key, value in article_stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nTop content words: {content_words[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.1: Word Frequency Analysis\n",
    "\n",
    "Create a simple word frequency counter for content words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def word_frequency_analysis(text, top_n=10):\n",
    "    \"\"\"\n",
    "    Analyze word frequency in text (content words only).\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        top_n (int): Number of top words to return\n",
    "        \n",
    "    Returns:\n",
    "        list: List of (word, frequency) tuples\n",
    "    \"\"\"\n",
    "    # TODO: Get content words and count their frequency\n",
    "    content_words = extract_content_words(text)\n",
    "    # TODO: Use Counter to count word frequencies\n",
    "    # TODO: Return the top_n most common words\n",
    "    \n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "# Test with the article\n",
    "top_words = word_frequency_analysis(article, top_n=5)\n",
    "print(\"Top 5 content words:\")\n",
    "for word, freq in top_words:\n",
    "    print(f\"  {word}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "\n",
    "Answer these questions based on what you've learned:\n",
    "\n",
    "1. **What is the relationship between `nlp`, `Doc`, and `Token` objects in spaCy?**\n",
    "\n",
    "2. **Why might you want to filter out stop words in text analysis?**\n",
    "\n",
    "3. **What are some advantages of using spaCy over simple string methods for text processing?**\n",
    "\n",
    "4. **How could the token attributes we explored be useful in real NLP applications?**\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations! You've completed Unit 1. You now understand:\n",
    "\n",
    "✅ How to load spaCy models and process text  \n",
    "✅ The hierarchy of spaCy objects (`nlp` → `Doc` → `Token`)  \n",
    "✅ Key token attributes like `text`, `lemma_`, `pos_`, `is_stop`  \n",
    "✅ How to filter and analyze tokens programmatically  \n",
    "✅ Basic text statistics and frequency analysis  \n",
    "\n",
    "**Next up:** [Unit 2 - Tokens & POS](../units/02-tokens-pos/index.md) where we'll dive deeper into part-of-speech tagging and linguistic analysis!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
